---
title: "Data Cleaning Process"
output:
  html_document: default
---
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Data context

- Company: an online retailer based in the UK. The business is fully e-commerce business without any physical store.
- Products: all-occasions gifts
- Customers: mainly wholesalers
- Data period: **1st of Dec, 2010 - 9nd of Dec, 2011**


## Data import

The first step of data cleaning tasks is to import the data. 'readr' package was used for the ease of importing data and changing the data type of 'Invoice' column to character, 'Quantity' column to integer, and 'InvoiceDate' column to date-time. 

```{r}
#Import CSV file using 'readr' package

library(readr)
online_retail_II <- read_csv("C:/Users/Tan/Desktop/kitty/R/R Projects/E-commerce Data/online_retail_II.csv", 
                             col_types = cols(Invoice = col_character(), 
                                              Quantity = col_integer(), InvoiceDate = col_datetime(format = "%d/%m/%Y %H.%M")))
head(online_retail_II)
```

## Change the name of 'Customer ID' column and Data summary

Using *summary()* function to have an overview at the dataset

```{r}
online_retail_II<- rename(online_retail_II, "CustomerID" = "Customer ID")
summary(online_retail_II)
```

Notice that there are **negative values** in 'Quantity' column, which is invalid. There are also **missing values** in 'CustomerID'.

## Deal with missing values and 'Quantity' negative values

**Strategy to deal with negative values and missing values**: Deleting the rows with negative values and missing values.

```{r}
#Remove rows with negative quantity
online_retail_II <- online_retail_II[online_retail_II$Quantity > 0,]
#Remove rows with missing values
online_retail_II <- na.omit(online_retail_II)
#Review the dataset
summary(online_retail_II)
```


## Redefine data types of variables

Change data type of 'Country', 'CustomerID', 'Invoice', 'StockCode', 'Description' variables to **Factor** using *as.factor()* function

```{r}
online_retail_II$Country <- as.factor(online_retail_II$Country)
online_retail_II$CustomerID <- as.factor(online_retail_II$CustomerID)
online_retail_II$Invoice <- as.factor(online_retail_II$Invoice)
online_retail_II$StockCode <- as.factor(online_retail_II$StockCode)
online_retail_II$Description <- as.factor(online_retail_II$Description)
```

## Add customize variables to the dataset

- Add 'Amount_Spent' column by multiply 'Quantity' with 'Price'
- Add 'Day_of_the_week', 'Month_YR', 'Hour' columns and change their data types to **Factor**

```{r}
#Add 'Amount_Spent' column
online_retail_II$Amount_spent <- online_retail_II$Quantity * online_retail_II$Price
#Add Day_of_the_week, Month_YR, Hour columns and change them to factor data type
online_retail_II$Day_of_the_week <- as.factor(weekdays(online_retail_II$InvoiceDate)) #there is no order on Saturday
online_retail_II$Month_Yr <- as.factor(format(online_retail_II$InvoiceDate, "%Y-%m"))
online_retail_II$Hour <- as.factor(format(online_retail_II$InvoiceDate, "%H"))
summary(online_retail_II)
```

## Redefine data types of variables

Change data type of 'Country', 'CustomerID', 'Invoice', 'StockCode', 'Description' variables to **Factor** using *as.factor()* function

```{r}
online_retail_II$Country <- as.factor(online_retail_II$Country)
online_retail_II$CustomerID <- as.factor(online_retail_II$CustomerID)
online_retail_II$Invoice <- as.factor(online_retail_II$Invoice)
online_retail_II$StockCode <- as.factor(online_retail_II$StockCode)
online_retail_II$Description <- as.factor(online_retail_II$Description)
```

## Managing duplicate data

The main goal of this step is to detect possible identical data and consider if they are duplicate or just normal identical data by chance.

```{r}
online_retail_II[duplicated(online_retail_II),]
```

It seems like there are 5,192 rows of duplicate data, however, considering the character of e-commerce dataset, it's possible and normal to have customer add the same product to the cart at the same time. Therefore, the identical data wasn't deleted out of the dataset.

## Managing outliers in the data

The goal of this step is to detect of outliers in the numeric or integer variables and consider if the outliers are relevant to the dataset or just typos. This report uses the **Six Sigma Method** for outliers detection.

**'Price' variable** outliers detection:

```{r}
x = online_retail_II$Price
t = 3
m = mean(x, na.rm = F)
s = sd(x, na.rm = F)
b1 = m-s*t
b2 = m+s*t
y = ifelse(x>= b1 & x<= b2, 0, 1)
#Plot for outliers detection of 'Price' variable
plot(x, col= y+2)
#Possible outliers
outl = which(y==1)
online_retail_II[outl,]
``` 

**'Quantity' variable** outliers detection:

```{r}
xs = online_retail_II$Quantity
ts = 3
ms = mean(x,na.rm = F)
ss = sd(x, na.rm = F)
b1s = ms - ss*ts
b2s = ms + ss*ts
ys = ifelse(xs>= b1s & xs<= b2s, 0,1)
#Plot for outliers detection of 'Quantity' variable
plot(xs,col= ys+2)
#Possible outliers
outls = which(ys==1)
online_retail_II[outls,]
```

There are 221 possible outliers for 'Price' variable and 10,594 possible outliers for 'Quantity' in the dataset. However, considering the characters of the e-commerce dataset, it's possible that there were customers ordered a huge quantity of products and there were also possible expensive products. Therefore, this report didn't exclude the possible outliers

## Plausibility check for Factor and Date-time variables

This step is to detect any invalid values of Factor and Date-time variables using *summary()* function.

```{r}
summary(online_retail_II)
```

There is no invalid factor or date-time values detected from the dataset.


